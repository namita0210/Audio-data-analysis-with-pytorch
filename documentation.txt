ABOUT SPEECH COMMANDS DATASET
__________________________________
Out of the available datasets in torchaudio.datasets I  have chosen the SPEECH COMMANDS dataset for the following reasons : -

1. small dataset
2. available annotations which specify each spoken word in the audio.
3. It is a dataset of 35 commands spoken by different people.
4. All audio files are about 1 second long

-- The actual loading and formatting steps happen when data point is being accessed, torchaudio converts the audio to tensors automatically.

-- If you want to load the audio file directly  use ```torchaudio.load()``` which returns a tuple containing newly created tensor along with sampling frequency of the audio file.

IMPORTS
_________________
** Framework - is a set of tools , libraries that provide a foundation for developing something. They also provide pre- built components and functionalities.

** module - set of self contained unit of software that encapsulates related functions, variables and other elements.

** interface - defines how the other parts of the program can interact with the functionality provided by a module without needing to know the internal implementation details.

- pytorch : Provides tensor computation (similar to NumPy) and GPU acceleration support.

- torch.nn : nn is a neural network module which includes pre defined layers, loss functions and other utilities for building neural networks.

- torch.nn.functional : Imports the functional interface of the nn module provides activation functions , convolutional operations and other functionalities that are commonly used in neural networks.

- optim : Contains optimization algorithms like SGD, Adam

- tqdm : Provides progress bars for loops and iterable tasks to monitor lengthy operations.

- torchaudio.functional : Contains various functional operations for audio processing like resampling , filtering etc.

- torchaudio.transforms : Contains transformations like spectrogram , time-frequency transformations.

SPLIT THE AUDIO DATA INTO TRAINING , TESTING , AND VALIDATION SUBSETS
______________________________________________________________________
The SubsetSC class is initialized with an optional argument subset, which specifies which subset of the data to load ("training", "testing", or "validation").

--> ``` def __init__(self , subset : str = None) : ```
This means that the constructor of this class is taking an argument subset of string type which is defaulted to None.
So, if the string is not passes as a parameter then the default value will be taken as None.

--> ``` super().__init__("./" , download = True) ```
This means that the constructor of the inherited class 'SPEECHCOMMANDS' is being called with the argument "./" which is the path to the directory where the data should be downloaded and that is also why download is set equal to true.

--> ```def load_list(filename):
           filepath = os.path.join(self._path , filename)

- os.path.join() is a function from the os.path module which is used to construct a path name by concatenating various path components.

- self._path refers to attribute named _path of the object instance.
Note that attributes prefixed with _ indicate that the attributes are intended to be private

- So, os.path.join(self._path, filename) constructs a full path to the file by joining the directory path (self._path) and the filename (filename). The resulting full path is stored in the variable filepath.

--> ``` with open(filepath) as fileobj :
             return [os.path.normpath(os.path.join(self._path, line.strip() ) ) for line in fileobj]
```
 - with statement is used in  python to acquire and release resources for the execution of a block of code that follows the with statement with an indentation.

- open(filepath) returns a file object which is captured by the variable fileobj.  Inside the indented block you can read from and write to the file using fileonj.

--> ``` return [os.path.normpath(os.path.join(self._path , line.strip() )) for line in fileobj] ```

- This is a list comprehension, the square brackets represent list comprehension.

- os.path.normpath normalizes a path name by collapsing redundant seperators. It takes a string as an input and returns a normalized version of that string 

- os.path.join(self._path , line.strip() )
This expression constructs a full path by joinng the directory path (self._path) and the stripped line from the line.strip()

  -- line.strip() removes whitespaces from each line in he fileobj

- return statement consists of a list of full paths to each line in the fileobj directory

--> ```if subset == "validation":
            self._walker = load_list("validation_list.txt")
        elif subset == "testing":
            self._walker = load_list("testing_list.txt")
        elif subset == "training":
            excludes = load_list("validation_list.txt") + load_list("testing_list.txt")
            excludes = set(excludes)
            self._walker = [w for w in self._walker if w not in excludes]```

- This code is a conditional block that sets the _walker attribute of an instance based on the value of the subset parameter passed to the class initializer.

- It then concatenates the two lists and converts the resulting list into a set to remove any duplicates (excludes = set(excludes)).

- Next, it filters the existing _walker list to exclude any paths that are present in the excludes set. This is done using a list comprehension that iterates over each path in _walker and checks if it is not present in the excludes set.


Inside the constructor (__init__ method) of SubsetSC:

It calls the constructor of the parent class SPEECHCOMMANDS, passing "./" as the path and download=True.
It defines a nested function load_list(filename) to load a list of file paths from a text file.
Depending on the value of the subset argument, it loads different lists of file paths:
If subset is "validation", it loads the validation list from "validation_list.txt".
If subset is "testing", it loads the testing list from "testing_list.txt".
If subset is "training", it loads the training list, excluding paths that appear in the validation and testing lists to avoid data leakage.
After defining the SubsetSC class, it creates instances of SubsetSC for training and testing data (train_set and test_set respectively) by passing "training" and "testing" as arguments to the constructor.

Finally, it accesses the first item in the training set (train_set[0]), which returns a tuple containing waveform data, sample rate, label, speaker ID, and utterance number.

Overall, this code snippet is designed to manage subsets of the Speech Commands dataset for training and testing machine learning models.

EXPLORING THE DATA
___________________
The Speech Commands dataset object returned by datasets.SPEECHCOMMANDS() does not have attributes like 'labels', 'speaker_ids', or 'utterance_numbers'
instead each sample in the dataset is a tuple containing the waveform tensor, sample rate, label (spoken word), speaker ID, and utterance number.

To explore the contents of the Speech Commands dataset, you typically access the data samples directly using indexing.

-- sample: This variable will be assigned the waveform tensor of the audio sample. It represents the actual audio data in tensor format, which you can analyze, preprocess, or use as input to machine learning models.

-- sample_rate: This variable will be assigned the sample rate of the audio sample. The sample rate indicates how many samples per second are included in the waveform tensor. It is typically measured in Hertz (Hz).

-- label: This variable will be assigned the label (spoken word) associated with the audio sample. For example, if the audio sample contains someone saying "yes", then the label will be "yes".

-- speaker_id: This variable will be assigned the ID of the speaker who uttered the audio sample. This information can be useful for tasks involving speaker recognition or diarization.

-- utterance_number: This variable will be assigned the utterance number of the audio sample. It identifies the specific instance of the spoken word within the dataset.

Overall, the line sample, sample_rate, label, speaker_id, utterance_number = data[0] retrieves the properties of the first audio sample in the Speech Commands dataset and assigns them to individual variables for further analysis or processing.

The shape of the tensor waveform of audio sample will be a one dimensional vector
of size equal to sampling rate.
For example if the sampling rate is 16000, then the size of a tensor will be
pytorch.size[1,16000] : This means that the tensor is one dimensional with a length of 16000
in other words it is a 1D vector with 16000 elements.

Note that It contains the amplitude values of the audio signal sampled over time.

FORMATTING THE DATA
____________________
This process is called downsampling. It's like squeezing the audio to make it take up less space, so the computer can process it faster.

Now, sometimes with other collections of sound recordings, you might have files that have more than one audio channel, like having sound come from both the left and right side of your headphones. But in our case, each recording we have only has one channel, which is perfect for what we need. So, we don't need to do anything special to handle multiple channels because there's only one channel to begin with. We can just focus on the single channel we have and work with that.

VISUALIZATION PARAMETERS
________________________

There are approx 100,000 audio samples in the dataset.

visualizing all of them would be impractical and overwhelming. 
Instead, you can select a subset of samples to visualize that represent 
the variety of spoken words, speakers, and recording conditions present in the dataset.

A few things to keep in mind for a holistic visualization are :-

-- Random Sampling: Randomly select a subset of samples from the dataset to visualize. 
This approach ensures that you get a diverse representation of the dataset without biasing towards specific words or speakers.

-- Stratified Sampling: If the dataset is labeled with spoken words (labels), you can perform stratified sampling to 
ensure that each spoken word is represented in the subset proportionally to its occurrence in the dataset. 
This helps ensure that you visualize samples from all spoken words in the dataset.

-- Speaker-Based Sampling:If the dataset includes speaker IDs, you can select samples from different speakers to visualize the
 variability in speech patterns and accents across speakers.

-- Keyword-Based Sampling: If you're interested in specific spoken words or keywords, you can select samples corresponding to those words
 for visualization. This can be useful for understanding how different words are pronounced and how their waveforms vary.

-- Environmental Conditions: Consider visualizing samples recorded under different environmental conditions,
 such as background noise levels, to understand how the audio quality varies across recordings.

-- Error Analysis: If you've already trained a model on the dataset, you can visualize samples that were misclassified or
 samples that the model struggled with. This can help identify challenging cases and areas for improvement.
















































