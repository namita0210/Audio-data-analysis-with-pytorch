ABOUT SPEECH COMMANDS DATASET
Out of the available datasets in torchaudio.datasets
I  have chosen the SPEECH COMMANDS dataset for the following reasons : -
1. small dataset
2. available anootations which specify each spoken word in the audio.
3. It is a dataset of 35 commands spoken by different people.
4. All audio files are about 1 second long

-- The actual loading and formatting steps happen when data point is being accessed, torchaudio converts the audio to tensors automatically.

-- If you want to load the audio file directly  use ```torchaudio.load()``` which returns a tuple containing newly created tensor along with sampling frequency of the audio file.

IMPORTS
** Framework - is a set of tools , libraries that provide a foundation for developing something. They also provide pre- built components and functionalities.

- optim : COntaines optimization algorithms like SGD, Adam
- tqdm : Provides progress bars for loops and iterable tasks to monitor lengthy operations.
- torchaudio.functional : Contains various functional operations for audio processing like resampling , filtering etc.
- torchaudio.transforms : Contains transformations like spectrogram , time-frequency transformations.

SPLIT THE AUDIO DATA INTO TRAINING , TESTING , AND VALIDATION SUBSETS
The SubsetSC class is initialized with an optional argument subset, which specifies which subset of the data to load ("training", "testing", or "validation").

Inside the constructor (__init__ method) of SubsetSC:

It calls the constructor of the parent class SPEECHCOMMANDS, passing "./" as the path and download=True.
It defines a nested function load_list(filename) to load a list of file paths from a text file.
Depending on the value of the subset argument, it loads different lists of file paths:
If subset is "validation", it loads the validation list from "validation_list.txt".
If subset is "testing", it loads the testing list from "testing_list.txt".
If subset is "training", it loads the training list, excluding paths that appear in the validation and testing lists to avoid data leakage.
After defining the SubsetSC class, it creates instances of SubsetSC for training and testing data (train_set and test_set respectively) by passing "training" and "testing" as arguments to the constructor.

Finally, it accesses the first item in the training set (train_set[0]), which returns a tuple containing waveform data, sample rate, label, speaker ID, and utterance number.

Overall, this code snippet is designed to manage subsets of the Speech Commands dataset for training and testing machine learning models.

EXPLORING THE DATA
The Speech Commands dataset object returned by datasets.SPEECHCOMMANDS() does not have attributes like 'labels', 'speaker_ids', or 'utterance_numbers'
instead each sample in the dataset is a tuple containing the waveform tensor, sample rate, label (spoken word), speaker ID, and utterance number.

To explore the contents of the Speech Commands dataset, you typically access the data samples directly using indexing.

-- sample: This variable will be assigned the waveform tensor of the audio sample. It represents the actual audio data in tensor format, which you can analyze, preprocess, or use as input to machine learning models.

-- sample_rate: This variable will be assigned the sample rate of the audio sample. The sample rate indicates how many samples per second are included in the waveform tensor. It is typically measured in Hertz (Hz).

-- label: This variable will be assigned the label (spoken word) associated with the audio sample. For example, if the audio sample contains someone saying "yes", then the label will be "yes".

-- speaker_id: This variable will be assigned the ID of the speaker who uttered the audio sample. This information can be useful for tasks involving speaker recognition or diarization.

-- utterance_number: This variable will be assigned the utterance number of the audio sample. It identifies the specific instance of the spoken word within the dataset.

Overall, the line sample, sample_rate, label, speaker_id, utterance_number = data[0] retrieves the properties of the first audio sample in the Speech Commands dataset and assigns them to individual variables for further analysis or processing.

The shape of the tensor waveform of audio sample will be a one dimensional vector
of size equal to sampling rate.
For example if the sampling rate is 16000, then the size of a tensor will be
pytorch.size[1,16000] : This means that the tensor is one dimensional with a length of 16000
in other words it is a 1D vector with 16000 elements.

Note that It contains the amplitude values of the audio signal sampled over time.

VISUALIZATION PARAMETERS
________________________

There are approx 100,000 audio samples in the dataset.

visualizing all of them would be impractical and overwhelming. 
Instead, you can select a subset of samples to visualize that represent 
the variety of spoken words, speakers, and recording conditions present in the dataset.

A few things to keep in mind for a holistic visualization are :-

-- Random Sampling: Randomly select a subset of samples from the dataset to visualize. 
This approach ensures that you get a diverse representation of the dataset without biasing towards specific words or speakers.

-- Stratified Sampling: If the dataset is labeled with spoken words (labels), you can perform stratified sampling to 
ensure that each spoken word is represented in the subset proportionally to its occurrence in the dataset. 
This helps ensure that you visualize samples from all spoken words in the dataset.

-- Speaker-Based Sampling:If the dataset includes speaker IDs, you can select samples from different speakers to visualize the
 variability in speech patterns and accents across speakers.

-- Keyword-Based Sampling: If you're interested in specific spoken words or keywords, you can select samples corresponding to those words
 for visualization. This can be useful for understanding how different words are pronounced and how their waveforms vary.

-- Environmental Conditions: Consider visualizing samples recorded under different environmental conditions,
 such as background noise levels, to understand how the audio quality varies across recordings.

-- Error Analysis: If you've already trained a model on the dataset, you can visualize samples that were misclassified or
 samples that the model struggled with. This can help identify challenging cases and areas for improvement.
















































